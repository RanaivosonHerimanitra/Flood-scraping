{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries I used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import httplib\n",
    "import urllib2\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "from httplib import IncompleteRead\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys  \n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "departement=['Bakel','Bambey','Bignona','Birkelane','Bounkiling','Dagana','Dakar','Diourbel','Fatick','Foundiougne',\n",
    "            'Gossas','Goudiry','Goudomp','Guediawaye','Guinguineo','Kaffrine','Kanel','Kaolack',\n",
    "             'Kebemer','Kedougou','Kolda','Koumpentoum','Koungheul','Linguere','Louga','Mbour',\n",
    "             'Malem Hodar','Matam','Mbacke','Medina Yoro Foulah','Nioro du Rip','Oussouye',\n",
    "             'Pikine','Podor','Ranerou-Ferlo','Rufisque','Saint-Louis','Salemata','Saraya',\n",
    "             'Sedhiou','Tambacounda','Thies','Tivaouane','Velingara','Ziguinchor']\n",
    "\n",
    "arrondissement = ['Almadies','Agnam Civol','Baba Garage','Bandafassi','Brassou','Bala','Bele','Boghal','Bona',\n",
    "                  'Boyngel Bamba','Bamba Thialene','Barkedji','Bembou','Bonkoto','Cas-Cas','Colobane','Coki',\n",
    "                  'Darou Minam','Dar Salam','Dakateli','Dagoudane','Dodji','Dakar Plateau',\n",
    "                  'Diakhao','Djilor','Djibanar','Djoulacolon','Diaroume','Darou Mousty','Dianke Makha','Diende',\n",
    "                  'Djibabouya','Djiredji','Fafacourou','Fissel','Fongolembi', 'Fimela',\n",
    "                  'Grand Dakar','Gamadji Sare','Guediawaye','Gniby',\n",
    "                  'Kataba','Kael','Kabrousse','Katakel','Karatanba','Koussanar','Keur Moussa',\n",
    "                  'Keniaba','Koulor','Koumbal','Kouthiaba Wolof','Keur Momar Sarr','Keur Mboucki',\n",
    "                  'Lambaye','Lour Escale','Loudia Ouoloff','Ida Mouride',\n",
    "                  'Mampatim','Makacolibantang','Mouderi','Mbane','Mbadakhoune','Missira Wadine','Missirah',\n",
    "                  'Meouane','Mbediene','Mabo','Medina Sabakh',\n",
    "                  'Ndindy','Ndoulo','Ndiedieng','Ndande','Ndame','Niodor','Ngoye','Ndorna','Niakhene',\n",
    "                  'Niaming','Niayes','Ndiaye','Niakhar','Niaguis','Notto','Nyassia',\n",
    "                  'Ogo','Orkadiere','Ouadiour', 'Parcelles Assainies','Pambal','Pakour','Paoskoto','Rao','Rufisque',\n",
    "                  'Sagatta','Sagatta Dioloff','Sare Bidji','Sare Coly Salle','Sindian','Sangalkam','Sagna','Sakal',\n",
    "                  'Sabodala','Sibassor','Sindia','Sessene','Salde','Thienaba','Thies',\n",
    "                  'Taif','Tendouck','Tenghory','Tattaguine','Thiaroye','Toubacouta','Thille Boubacar',\n",
    "                  'Velingara','Wouro Sidy','Wack Ngouna','Yang Yang']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load stored data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "site_web=[\"http://www.aps.sn\",\"http://www.walf-groupe.com\",\"http://www.sudonline.sn\",\n",
    "          \"http://www.lesoleil.sn\",\"http://www.lequotidien.sn\",\"http://xalimasn.com\",\n",
    "          \"http://www.seneweb.com/index.php\",\"http://www.lagazette.sn\",\n",
    "          \"http://www.senetoile.net\",\"http://senego.com\",\n",
    "          \"http://www.sen360.com\",\"http://www.senenews.com\",\n",
    "          \"http://www.sen24heures.com\",\n",
    "          \"http://www.nettali.net\",\"http://www.leuksenegal.com\",\n",
    "          \"http://www.leral.net\",\"http://www.au-senegal.com/?lang=fr\",\n",
    "          \"http://www.dakaractu.com\",\"http://actusen.com\",\n",
    "          \"http://www.devoircitoyen.fr\",\n",
    "          \"http://www.homeviewsenegal.com\",\"http://www.enqueteplus.com\",\n",
    "          \"http://www.gouv.sn\",\"http://www.jaaduwul.com\",\"http://www.sn.undp.org\",\n",
    "          \"http://www.senegaltribune.com/articles\",\"http://www.slateafrique.com\",\n",
    "          \"http://www.caritas.sn\",\"http://senegal-business.com\",\n",
    "          \"http://xibaaru.com\",\"http://www.xibar.net\",\"http://www.lifixew.com\",\n",
    "          \"http://annagueye.blogspot.com\",\"http://www.ferloo.com\",\n",
    "          \"http://www.derniereminute.sn\",\"http://www.sunuker.com\",\"http://laviesenegalaise.net\",\n",
    "          \"http://mathiamenvironmentsenegal.blogspot.com\",\"http://lasomone.com\",\n",
    "          \"http://ledakarois.net\",\"http://www.dakar-echo.com\",\"http://www.telesenegal.com\",\n",
    "          \"http://www.igfm.sn\",\"http://galsen221.com\",\"http://www.dakarposte.com\",\n",
    "          \"http://mediafrik.com\",\"http://www.planete-senegal.com\",\"http://allodakar.com\",\n",
    "          \"http://www.ndarinfo.com\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_page=pd.read_csv(\"all_page.csv\",sep=\";\")\n",
    "all_page=list(all_page.link)\n",
    "all_page1=pd.read_csv(\"base_links.csv\",sep=\";\")\n",
    "all_page1=list(all_page1.link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Iteration 2: Continuously loop against links obtained at [iteration1](https://github.com/RanaivosonHerimanitra/Flood-scraping/blob/master/iteration1.ipynb) to attain archives\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### randomly sample to introduce randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_page= random.sample(all_page,len(all_page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ea860f75bbce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#handle page that does not actually exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mmypage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmypage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"the above page does not exist\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36m_read_chunked\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mchunk_left\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m                 \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk_left\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;31m# fragmentation issues on many platforms.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i,s in enumerate(all_page):\n",
    "    print i, s\n",
    "    try:\n",
    "        if \"pdf\" not in s and \"jpg\" not in s and \"javascript\" not in s:\n",
    "            req = urllib2.Request(s.decode('utf-8'), headers={'User-Agent' : \"Magic Browser\"})\n",
    "            page = urllib2.urlopen( req )\n",
    "        else:\n",
    "            continue\n",
    "    except urllib2.HTTPError,e:\n",
    "        continue\n",
    "    except urllib2.URLError, e:\n",
    "        continue\n",
    "    except httplib.HTTPException, e:\n",
    "        continue\n",
    "    except Exception:\n",
    "        continue\n",
    "    #handle page that does not actually exist\n",
    "    mypage=page.read()\n",
    "    if len(mypage)==0:\n",
    "        print \"the above page does not exist\"\n",
    "        continue\n",
    "    else:\n",
    "        soup = BeautifulSoup(mypage)\n",
    "        #find all links from the current \"page\"\n",
    "        all_links= soup.find_all(\"a\")\n",
    "        #ensure all links are valid\n",
    "        link_inside= [link.get(\"href\") for link in all_links if link.get(\"href\") is not None]\n",
    "        link_inside= [x for x in link_inside if 'www.' in x or 'http://' in x]\n",
    "        #do not append links from starter links\n",
    "        link_inside = [link for link in link_inside if link not in site_web]\n",
    "        #do not append links from previous records\n",
    "        link_inside = [link for link in link_inside if link not in all_page]\n",
    "        #do not append links in the current records\n",
    "        link_inside = [link for link in link_inside if link not in all_page1]\n",
    "        for k in np.unique(link_inside):\n",
    "            print k, len(all_page1)\n",
    "            all_page1.append(k)\n",
    "    #save on the fly\n",
    "    df = pd.DataFrame(data=all_page1, index= range(len(all_page1)), columns=['link'])\n",
    "    df.to_csv(\"base_links.csv\",sep=\";\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep unique pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92741"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_page1=np.unique(all_page1)\n",
    "all_page1=list(all_page1)\n",
    "len(all_page1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter to remove social media links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85412"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=filter(lambda x: 'linkedin' in x or 'facebook' in x or 'twitter' in x or 'youtube' in x or 'http://www.meteo-senegal.net' in x, all_page1)\n",
    "for k in site_web:\n",
    "    f.append(k)\n",
    "all_page1= [x for x in all_page1 if x not in f]\n",
    "len(all_page1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load data (base links) again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=all_page1, index= range(len(all_page1)), columns=['link'])\n",
    "df.to_csv(\"base_links.csv\",sep=\";\",encoding=\"utf-8\")\n",
    "all_page1=pd.read_csv(\"base_links.csv\",sep=\";\")\n",
    "all_page1=list(all_page1.link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for keywords related to flood events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "490"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flood_kwds='(noyé|toit population loyer quartier|meteo|evacués|evacuation|SOS au secours|crise humanitaire|sinistres|'\n",
    "flood_kwds=flood_kwds+'diluvienne|précipitations|pluviométri|bidonville'\n",
    "flood_kwds=flood_kwds+'montée des eaux|flood|zone risque|remblayage|effondrement|engorgement|débordé|digue'\n",
    "flood_kwds=flood_kwds+'|submergé|éclatement barrage|rupture barrage|glissement terrain|climatique|inondé|inondables|'\n",
    "#flood_kwds= flood_kwds+ '|'.join(departement)\n",
    "flood_kwds= flood_kwds+'inondation|inondations|pluies|pluie|banlieu|bas-quartiers)'\n",
    "\n",
    "#2nd iter\n",
    "flood_page_from_all_page1= [x for x in all_page1 if re.search(flood_kwds,x) is not None]\n",
    "#1st iter\n",
    "flood_page_from_all_page= [x for x in all_page if re.search(flood_kwds,x) is not None]\n",
    "flood_page=flood_page_from_all_page + flood_page_from_all_page1\n",
    "flood_page=np.unique(flood_page)\n",
    "len(flood_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(490, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=flood_page, index= range(len(flood_page)), columns=['link'])\n",
    "df.to_csv(\"potential_flood_events.csv\",sep=\";\",encoding=\"utf-8\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
