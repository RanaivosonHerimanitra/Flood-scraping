{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries I used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import httplib\n",
    "import urllib2\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "from httplib import IncompleteRead\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys  \n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "departement=['Bakel','Bambey','Bignona','Birkelane','Bounkiling','Dagana','Dakar','Diourbel','Fatick','Foundiougne',\n",
    "            'Gossas','Goudiry','Goudomp','Guediawaye','Guinguineo','Kaffrine','Kanel','Kaolack',\n",
    "             'Kebemer','Kedougou','Kolda','Koumpentoum','Koungheul','Linguere','Louga','Mbour',\n",
    "             'Malem Hodar','Matam','Mbacke','Medina Yoro Foulah','Nioro du Rip','Oussouye',\n",
    "             'Pikine','Podor','Ranerou-Ferlo','Rufisque','Saint-Louis','Salemata','Saraya',\n",
    "             'Sedhiou','Tambacounda','Thies','Tivaouane','Velingara','Ziguinchor']\n",
    "\n",
    "arrondissement = ['Almadies','Agnam Civol','Baba Garage','Bandafassi','Brassou','Bala','Bele','Boghal','Bona',\n",
    "                  'Boyngel Bamba','Bamba Thialene','Barkedji','Bembou','Bonkoto','Cas-Cas','Colobane','Coki',\n",
    "                  'Darou Minam','Dar Salam','Dakateli','Dagoudane','Dodji','Dakar Plateau',\n",
    "                  'Diakhao','Djilor','Djibanar','Djoulacolon','Diaroume','Darou Mousty','Dianke Makha','Diende',\n",
    "                  'Djibabouya','Djiredji','Fafacourou','Fissel','Fongolembi', 'Fimela',\n",
    "                  'Grand Dakar','Gamadji Sare','Guediawaye','Gniby',\n",
    "                  'Kataba','Kael','Kabrousse','Katakel','Karatanba','Koussanar','Keur Moussa',\n",
    "                  'Keniaba','Koulor','Koumbal','Kouthiaba Wolof','Keur Momar Sarr','Keur Mboucki',\n",
    "                  'Lambaye','Lour Escale','Loudia Ouoloff','Ida Mouride',\n",
    "                  'Mampatim','Makacolibantang','Mouderi','Mbane','Mbadakhoune','Missira Wadine','Missirah',\n",
    "                  'Meouane','Mbediene','Mabo','Medina Sabakh',\n",
    "                  'Ndindy','Ndoulo','Ndiedieng','Ndande','Ndame','Niodor','Ngoye','Ndorna','Niakhene',\n",
    "                  'Niaming','Niayes','Ndiaye','Niakhar','Niaguis','Notto','Nyassia',\n",
    "                  'Ogo','Orkadiere','Ouadiour', 'Parcelles Assainies','Pambal','Pakour','Paoskoto','Rao','Rufisque',\n",
    "                  'Sagatta','Sagatta Dioloff','Sare Bidji','Sare Coly Salle','Sindian','Sangalkam','Sagna','Sakal',\n",
    "                  'Sabodala','Sibassor','Sindia','Sessene','Salde','Thienaba','Thies',\n",
    "                  'Taif','Tendouck','Tenghory','Tattaguine','Thiaroye','Toubacouta','Thille Boubacar',\n",
    "                  'Velingara','Wouro Sidy','Wack Ngouna','Yang Yang']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load stored data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "site_web=[\"http://www.aps.sn\",\"http://www.walf-groupe.com\",\"http://www.sudonline.sn\",\n",
    "          \"http://www.lesoleil.sn\",\"http://www.lequotidien.sn\",\"http://xalimasn.com\",\n",
    "          \"http://www.seneweb.com/index.php\",\"http://www.lagazette.sn\",\n",
    "          \"http://www.senetoile.net\",\"http://senego.com\",\n",
    "          \"http://www.sen360.com\",\"http://www.senenews.com\",\n",
    "          \"http://www.sen24heures.com\",\n",
    "          \"http://www.nettali.net\",\"http://www.leuksenegal.com\",\n",
    "          \"http://www.leral.net\",\"http://www.au-senegal.com/?lang=fr\",\n",
    "          \"http://www.dakaractu.com\",\"http://actusen.com\",\n",
    "          \"http://www.devoircitoyen.fr\",\n",
    "          \"http://www.homeviewsenegal.com\",\"http://www.enqueteplus.com\",\n",
    "          \"http://www.gouv.sn\",\"http://www.jaaduwul.com\",\"http://www.sn.undp.org\",\n",
    "          \"http://www.senegaltribune.com/articles\",\"http://www.slateafrique.com\",\n",
    "          \"http://www.caritas.sn\",\"http://senegal-business.com\",\n",
    "          \"http://xibaaru.com\",\"http://www.xibar.net\",\"http://www.lifixew.com\",\n",
    "          \"http://annagueye.blogspot.com\",\"http://www.ferloo.com\",\n",
    "          \"http://www.derniereminute.sn\",\"http://www.sunuker.com\",\"http://laviesenegalaise.net\",\n",
    "          \"http://mathiamenvironmentsenegal.blogspot.com\",\"http://lasomone.com\",\n",
    "          \"http://ledakarois.net\",\"http://www.dakar-echo.com\",\"http://www.telesenegal.com\",\n",
    "          \"http://www.igfm.sn\",\"http://galsen221.com\",\"http://www.dakarposte.com\",\n",
    "          \"http://mediafrik.com\",\"http://www.planete-senegal.com\",\"http://allodakar.com\",\n",
    "          \"http://www.ndarinfo.com\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_page=pd.read_csv(\"all_page.csv\",sep=\";\")\n",
    "all_page=list(all_page.link)\n",
    "all_page1=pd.read_csv(\"base_links.csv\",sep=\";\")\n",
    "all_page1=list(all_page1.link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Iteration 2: Continuously loop against links obtained at [iteration1](https://github.com/RanaivosonHerimanitra/Flood-scraping/blob/master/iteration1.ipynb) to attain archives\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### randomly sample to introduce randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_page= random.sample(all_page,len(all_page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,s in enumerate(all_page):\n",
    "    print i, s\n",
    "    try:\n",
    "        if \"pdf\" not in s and \"jpg\" not in s and \"javascript\" not in s:\n",
    "            req = urllib2.Request(s.decode('utf-8'), headers={'User-Agent' : \"Magic Browser\"})\n",
    "            page = urllib2.urlopen( req )\n",
    "        else:\n",
    "            continue\n",
    "    except urllib2.HTTPError,e:\n",
    "        continue\n",
    "    except urllib2.URLError, e:\n",
    "        continue\n",
    "    except httplib.HTTPException, e:\n",
    "        continue\n",
    "    except Exception:\n",
    "        continue\n",
    "    #handle page that does not actually exist\n",
    "    mypage=page.read()\n",
    "    if len(mypage)==0:\n",
    "        print \"the above page does not exist\"\n",
    "        continue\n",
    "    else:\n",
    "        soup = BeautifulSoup(mypage)\n",
    "        #find all links from the current \"page\"\n",
    "        all_links= soup.find_all(\"a\")\n",
    "        #ensure all links are valid\n",
    "        link_inside= [link.get(\"href\") for link in all_links if link.get(\"href\") is not None]\n",
    "        link_inside= [x for x in link_inside if 'www.' in x or 'http://' in x]\n",
    "        #do not append links from starter links\n",
    "        link_inside = [link for link in link_inside if link not in site_web]\n",
    "        #do not append links from previous records\n",
    "        link_inside = [link for link in link_inside if link not in all_page]\n",
    "        #do not append links in the current records\n",
    "        link_inside = [link for link in link_inside if link not in all_page1]\n",
    "        for k in np.unique(link_inside):\n",
    "            print k, len(all_page1)\n",
    "            all_page1.append(k)\n",
    "    #save on the fly\n",
    "    df = pd.DataFrame(data=all_page1, index= range(len(all_page1)), columns=['link'])\n",
    "    df.to_csv(\"base_links.csv\",sep=\";\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep unique pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57119"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_page1=np.unique(all_page1)\n",
    "all_page1=list(all_page1)\n",
    "len(all_page1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter to remove social media links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54885"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=filter(lambda x: 'linkedin' in x or 'facebook' in x or 'twitter' in x or 'youtube' in x or 'http://www.meteo-senegal.net' in x, all_page1)\n",
    "for k in site_web:\n",
    "    f.append(k)\n",
    "all_page1= [x for x in all_page1 if x not in f]\n",
    "len(all_page1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load data (base links) again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=all_page1, index= range(len(all_page1)), columns=['link'])\n",
    "df.to_csv(\"base_links.csv\",sep=\";\",encoding=\"utf-8\")\n",
    "all_page1=pd.read_csv(\"base_links.csv\",sep=\";\")\n",
    "all_page1=list(all_page1.link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for keywords related to flood events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flood_kwds='(noyé|toit population loyer quartier|meteo|evacués|evacuation|SOS au secours|crise humanitaire|sinistres|'\n",
    "flood_kwds=flood_kwds+'diluvienne|précipitations|pluviométri|bidonville'\n",
    "flood_kwds=flood_kwds+'montée des eaux|flood|zone risque|remblayage|effondrement|engorgement|débordé|digue'\n",
    "flood_kwds=flood_kwds+'|submergé|éclatement barrage|rupture barrage|glissement terrain|climatique|inondé|inondables|'\n",
    "#flood_kwds= flood_kwds+ '|'.join(departement)\n",
    "flood_kwds= flood_kwds+'inondation|inondations|pluies|pluie|banlieu|bas-quartiers)'\n",
    "\n",
    "#2nd iter\n",
    "flood_page_from_all_page1= [x for x in all_page1 if re.search(flood_kwds,x) is not None]\n",
    "#1st iter\n",
    "flood_page_from_all_page= [x for x in all_page if re.search(flood_kwds,x) is not None]\n",
    "flood_page=flood_page_from_all_page + flood_page_from_all_page1\n",
    "flood_page=np.unique(flood_page)\n",
    "len(flood_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(344, 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=flood_page, index= range(len(flood_page)), columns=['link'])\n",
    "df.to_csv(\"potential_flood_events.csv\",sep=\";\",encoding=\"utf-8\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
