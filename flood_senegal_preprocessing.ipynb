{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries I used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import httplib\n",
    "import urllib2\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "from httplib import IncompleteRead\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys  \n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load stored data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_page=pd.read_csv(\"all_page.csv\",sep=\";\")\n",
    "all_page=list(all_page.link)\n",
    "all_page1=pd.read_csv(\"base_links.csv\",sep=\";\")\n",
    "all_page1=list(all_page1.link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Iteration 2: Continuously loop against links obtained at [iteration1](https://github.com/RanaivosonHerimanitra/Flood-scraping/blob/master/iteration1.ipynb) to attain archives\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### randomly sample to introduce randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_page= random.sample(all_page,len(all_page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-67dbcb835676>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mlink_inside\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlink\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlink_inside\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_page\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m#do not append links in the current records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mlink_inside\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlink\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlink_inside\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_page1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink_inside\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_page1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i,s in enumerate(all_page):\n",
    "    print i, s\n",
    "    try:\n",
    "        if \"pdf\" not in s and \"jpg\" not in s and \"javascript\" not in s:\n",
    "            req = urllib2.Request(s.decode('utf-8'), headers={'User-Agent' : \"Magic Browser\"})\n",
    "            page = urllib2.urlopen( req )\n",
    "        else:\n",
    "            continue\n",
    "    except urllib2.HTTPError,e:\n",
    "        continue\n",
    "    except urllib2.URLError, e:\n",
    "        continue\n",
    "    except httplib.HTTPException, e:\n",
    "        continue\n",
    "    except Exception:\n",
    "        continue\n",
    "    #handle page that does not actually exist\n",
    "    mypage=page.read()\n",
    "    if len(mypage)==0:\n",
    "        print \"the above page does not exist\"\n",
    "        continue\n",
    "    else:\n",
    "        soup = BeautifulSoup(mypage)\n",
    "        #find all links from the current \"page\"\n",
    "        all_links= soup.find_all(\"a\")\n",
    "        #ensure all links are valid\n",
    "        link_inside= [link.get(\"href\") for link in all_links if link.get(\"href\") is not None]\n",
    "        link_inside= [link if link.startswith('http') or link.startswith('www') else s + link for link in link_inside]\n",
    "        #do not append links from starter links\n",
    "        link_inside = [link for link in link_inside if link not in site_web]\n",
    "        #do not append links from previous records\n",
    "        link_inside = [link for link in link_inside if link not in all_page]\n",
    "        #do not append links in the current records\n",
    "        link_inside = [link for link in link_inside if link not in all_page1]\n",
    "        for k in np.unique(link_inside):\n",
    "            print k, len(all_page1)\n",
    "            all_page1.append(k)\n",
    "    #save on the fly\n",
    "    df = pd.DataFrame(data=all_page1, index= range(len(all_page1)), columns=['link'])\n",
    "    df.to_csv(\"base_links.csv\",sep=\";\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep unique pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92197"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_page1=np.unique(all_page1)\n",
    "all_page1=list(all_page1)\n",
    "len(all_page1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter to remove social media links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91875"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=filter(lambda x: 'linkedin' in x or 'facebook' in x or 'twitter' in x or 'youtube' in x or 'http://www.meteo-senegal.net' in x, all_page1)\n",
    "for k in site_web:\n",
    "    f.append(k)\n",
    "all_page1= [x for x in all_page1 if x not in f]\n",
    "len(all_page1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data (base links) again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_page1=pd.read_csv(\"base_links.csv\",sep=\";\")\n",
    "all_page1=list(all_page1.link)\n",
    "all_page=pd.read_csv(\"all_page.csv\",sep=\";\")\n",
    "all_page=list(all_page.link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for keywords related to flood events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flood_kwds='(meteo|evacués|evacuation|SOS|crise humanitaire|sinistres|précipitations|pluviométri|bidonville|'\n",
    "flood_kwds=flood_kwds+'montée des eaux|flood|zone risque|remblayage|effondrement|engorgement|débordé|digue'\n",
    "flood_kwds=flood_kwds+'|submergé|éclatement barrage|rupture barrage|glissement terrain|climatique|inondé|inondables|'\n",
    "flood_kwds= flood_kwds+'inondation|inondations|pluies|pluie|banlieu|bas-quartiers)'\n",
    "#2nd iter\n",
    "flood_page_from_all_page1= [x for x in all_page1 if re.search(flood_kwds,x) is not None]\n",
    "#1st iter\n",
    "flood_page_from_all_page= [x for x in all_page if re.search(flood_kwds,x) is not None]\n",
    "flood_page=flood_page_from_all_page + flood_page_from_all_page1\n",
    "flood_page=np.unique(flood_page)\n",
    "len(flood_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results ==>262 links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=flood_page, index= range(len(flood_page)), columns=['link'])\n",
    "df.to_csv(\"potential_flood_events.csv\",sep=\";\",encoding=\"utf-8\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
