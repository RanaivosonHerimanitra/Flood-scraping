{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining on relevant pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Libraries used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import httplib\n",
    "import urllib2\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "from httplib import IncompleteRead\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys  \n",
    "#reload(sys)  \n",
    "#sys.setdefaultencoding('latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "departement=['Bakel','Bambey','Bignona','Birkelane','Bounkiling','Dagana','Dakar','Diourbel','Fatick','Foundiougne',\n",
    "            'Gossas','Goudiry','Goudomp','Guediawaye','Guinguineo','Kaffrine','Kanel','Kaolack',\n",
    "             'Kebemer','Kedougou','Kolda','Koumpentoum','Koungheul','Linguere','Louga','Mbour',\n",
    "             'Malem Hodar','Matam','Mbacke','Medina Yoro Foulah','Nioro du Rip','Oussouye',\n",
    "             'Pikine','Podor','Ranerou-Ferlo','Rufisque','Saint-Louis','Salemata','Saraya',\n",
    "             'Sedhiou','Tambacounda','Thies','Tivaouane','Velingara','Ziguinchor']\n",
    "\n",
    "arrondissement = ['Almadies','Agnam Civol','Baba Garage','Bandafassi','Brassou','Bala','Bele','Boghal','Bona',\n",
    "                  'Boyngel Bamba','Bamba Thialene','Barkedji','Bembou','Bonkoto','Cas-Cas','Colobane','Coki',\n",
    "                  'Darou Minam','Dar Salam','Dakateli','Dagoudane','Dodji','Dakar Plateau',\n",
    "                  'Diakhao','Djilor','Djibanar','Djoulacolon','Diaroume','Darou Mousty','Dianke Makha','Diende',\n",
    "                  'Djibabouya','Djiredji','Fafacourou','Fissel','Fongolembi', 'Fimela',\n",
    "                  'Grand Dakar','Gamadji Sare','Guediawaye','Gniby',\n",
    "                  'Kataba','Kael','Kabrousse','Katakel','Karatanba','Koussanar','Keur Moussa',\n",
    "                  'Keniaba','Koulor','Koumbal','Kouthiaba Wolof','Keur Momar Sarr','Keur Mboucki',\n",
    "                  'Lambaye','Lour Escale','Loudia Ouoloff','Ida Mouride',\n",
    "                  'Mampatim','Makacolibantang','Mouderi','Mbane','Mbadakhoune','Missira Wadine','Missirah',\n",
    "                  'Meouane','Mbediene','Mabo','Medina Sabakh',\n",
    "                  'Ndindy','Ndoulo','Ndiedieng','Ndande','Ndame','Niodor','Ngoye','Ndorna','Niakhene',\n",
    "                  'Niaming','Niayes','Ndiaye','Niakhar','Niaguis','Notto','Nyassia',\n",
    "                  'Ogo','Orkadiere','Ouadiour', 'Parcelles Assainies','Pambal','Pakour','Paoskoto','Rao','Rufisque',\n",
    "                  'Sagatta','Sagatta Dioloff','Sare Bidji','Sare Coly Salle','Sindian','Sangalkam','Sagna','Sakal',\n",
    "                  'Sabodala','Sibassor','Sindia','Sessene','Salde','Thienaba','Thies',\n",
    "                  'Taif','Tendouck','Tenghory','Tattaguine','Thiaroye','Toubacouta','Thille Boubacar',\n",
    "                  'Velingara','Wouro Sidy','Wack Ngouna','Yang Yang']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"potential_flood_events.csv\",sep=\";\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the story behind links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['story']=''\n",
    "def append_story_todf():\n",
    "    for i , url in enumerate(df.link):\n",
    "        try:\n",
    "            req = urllib2.Request(url.decode('utf-8'), headers={'User-Agent' : \"Magic Browser\"})\n",
    "            page = urllib2.urlopen( req )\n",
    "        except urllib2.HTTPError,e:\n",
    "            continue\n",
    "        except urllib2.URLError, e:\n",
    "            continue\n",
    "        except httplib.HTTPException, e:\n",
    "            continue\n",
    "        except Exception:\n",
    "            continue\n",
    "        mypage=page.read()\n",
    "        if len(mypage)==0:\n",
    "            print \"the above page does not exist\"\n",
    "            continue\n",
    "        else:\n",
    "            soup = BeautifulSoup(mypage)\n",
    "            #find all paragraphs from the current \"page\"\n",
    "            all_para=[]\n",
    "            components= soup.find_all('time') + soup.find_all('p')\n",
    "            for para in components:\n",
    "                all_para.append(para.text)\n",
    "                df.loc[i,'story']= ' '.join(all_para).strip()  \n",
    "append_story_todf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter relevancy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#main subject on the link +1\n",
    "df['relevancy'] = [1 if re.search(\"(inondation|inondes)\",x) is not None else 0 for x in df.link  ]\n",
    "# main subject on the content story +1\n",
    "df['relevancy'] += [1 if re.search(\"(inondation|inondes|sinistre)\",x) is not None else 0 for x in df.story]\n",
    "# mention of any department on content +1:\n",
    "df['relevancy'] +=[ 1 if x in departement else 0 for x in df.story ]\n",
    "# mention of any arrondissement on content +1:\n",
    "df['relevancy'] +=[ 1 if x in arrondissement else 0 for x in df.story  ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get place by departement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['place']=''\n",
    "for i, j in enumerate(df['link']):\n",
    "    x= [ k for k in departement if k.lower() in j  ]\n",
    "    if x!=[]:\n",
    "        if len(x)==1:\n",
    "            df.loc[i,'place']=str(x)\n",
    "        if len(x)>1:\n",
    "            df.loc[i,'place']=\"-\".join(x)\n",
    "    else:\n",
    "        df.loc[i,'place']=''\n",
    "df.place.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search place in story content (not yet mature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, j in enumerate(df['story']):\n",
    "    x= [ k for k in departement if k.lower() in j and df.loc[i,'place']==''  ]\n",
    "    if x!=[]:\n",
    "        if len(x)==1:\n",
    "            df.loc[i,'place']=str(x)\n",
    "        if len(x)>1:\n",
    "            df.loc[i,'place']=\"-\".join(x)\n",
    "df.place.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mean number of character of all the links\n",
    "# slice based on median\n",
    "#drop duplicates based on those shorten links\n",
    "# but before decrease until duplicates appear\n",
    "z=[len(x) for x in df.link]\n",
    "median_link_index= int(round(np.median(z)))\n",
    "L=len(df.link)\n",
    "while L == len(df.link):\n",
    "    df['link_median']= df['link'].apply(lambda x: x[:median_link_index])\n",
    "    L=len(df.link_median.unique())\n",
    "    median_link_index= median_link_index - 10\n",
    "df=df.drop_duplicates(subset=[\"link_median\"])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df.relevancy>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df['date']= [ re.search(\"publie [a-z-0-9]\",x)[:10] if re.search(\"publie [a-z-0-9]\",x) is not None else '' for x in df.story]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
