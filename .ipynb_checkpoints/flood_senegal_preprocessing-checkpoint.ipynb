{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries I used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import httplib\n",
    "import urllib2\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "from httplib import IncompleteRead\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys  \n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of relevant newpapers in Senegal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "site_web=[\"http://www.aps.sn\",\"http://www.walf-groupe.com\",\"http://www.sudonline.sn\",\n",
    "          \"http://www.lesoleil.sn\",\"http://www.lequotidien.sn\",\"http://xalimasn.com\",\n",
    "          \"http://www.seneweb.com/index.php\",\"http://www.lagazette.sn\",\n",
    "          \"http://www.senetoile.net\",\"http://senego.com\",\n",
    "          \"http://www.sen360.com\",\"http://www.senenews.com\",\n",
    "          \"http://www.sen24heures.com\",\n",
    "          \"http://www.nettali.net\",\"http://www.leuksenegal.com\",\n",
    "          \"http://www.leral.net\",\"http://www.au-senegal.com/?lang=fr\",\n",
    "          \"http://www.dakaractu.com\",\"http://actusen.com\",\n",
    "          \"http://www.devoircitoyen.fr\",\n",
    "          \"http://www.homeviewsenegal.com\",\"http://www.enqueteplus.com\",\n",
    "          \"http://www.gouv.sn\",\"http://www.jaaduwul.com\",\"http://www.sn.undp.org\",\n",
    "          \"http://www.senegaltribune.com/articles\",\"http://www.slateafrique.com\",\n",
    "          \"http://www.caritas.sn\",\"http://senegal-business.com\",\n",
    "          \"http://xibaaru.com\",\"http://www.xibar.net\",\"http://www.lifixew.com\",\n",
    "          \"http://annagueye.blogspot.com\",\"http://www.ferloo.com\",\n",
    "          \"http://www.derniereminute.sn\",\"http://www.sunuker.com\",\"http://laviesenegalaise.net\",\n",
    "          \"http://mathiamenvironmentsenegal.blogspot.com\",\"http://lasomone.com\",\n",
    "          \"http://ledakarois.net\",\"http://www.dakar-echo.com\",\"http://www.telesenegal.com\",\n",
    "          \"http://www.igfm.sn\",\"http://galsen221.com\",\"http://www.dakarposte.com\",\n",
    "          \"http://mediafrik.com\",\"http://www.planete-senegal.com\",\"http://allodakar.com\",\n",
    "          \"http://www.ndarinfo.com\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interation1 : loop thru newspapers and extract all links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_page1=pd.read_csv(\"base_links.csv\",sep=\";\")\n",
    "all_page1=list(all_page1.link)\n",
    "all_page=pd.read_csv(\"all_page.csv\",sep=\";\")\n",
    "all_page=list(all_page.link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all_page=[]\n",
    "for i,s in enumerate(site_web):\n",
    "    print i,s\n",
    "    try:\n",
    "        req = urllib2.Request(s.decode('utf-8'), headers={'User-Agent' : \"Magic Browser\"}) \n",
    "        page = urllib2.urlopen( req )\n",
    "        soup = BeautifulSoup(page)\n",
    "    except urllib2.HTTPError,e:\n",
    "        continue\n",
    "    except urllib2.URLError, e:\n",
    "        continue\n",
    "    except httplib.HTTPException, e:\n",
    "        continue\n",
    "    except Exception:\n",
    "        continue\n",
    "    #find all links\n",
    "    all_links= soup.find_all(\"a\")\n",
    "    #ensure all links are valid\n",
    "    link_inside= [link.get(\"href\") for link in all_links if link.get(\"href\") is not None]\n",
    "    link_inside= [link if link.startswith('http') or link.startswith('www') else s + link for link in link_inside]\n",
    "    #do not keep links from starter links\n",
    "    link_inside = [link for link in link_inside if link not in site_web]\n",
    "    #do not keep links that have been already recorded from the current loop\n",
    "    link_inside = [link for link in link_inside if link not in all_page]\n",
    "    for k in np.unique(link_inside):\n",
    "        all_page.append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Keep unique obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15428"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_page=np.unique(all_page)\n",
    "len(all_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save \"all_page\" variable in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=all_page, index= range(len(all_page)), columns=['link'])\n",
    "df.to_csv(\"all_page.csv\",sep=\";\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 2: loop again from the obtained links to add more depth and attain archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_page1=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## randomly sample \"all_page\" to introduce randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_page= random.sample(all_page,len(all_page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,s in enumerate(all_page[1580:]):\n",
    "    print i, s\n",
    "    try:\n",
    "        if \"pdf\" not in s and \"jpg\" not in s and \"javascript\" not in s:\n",
    "            req = urllib2.Request(s.decode('utf-8'), headers={'User-Agent' : \"Magic Browser\"})\n",
    "            page = urllib2.urlopen( req )\n",
    "        else:\n",
    "            continue\n",
    "    except urllib2.HTTPError,e:\n",
    "        continue\n",
    "    except urllib2.URLError, e:\n",
    "        continue\n",
    "    except httplib.HTTPException, e:\n",
    "        continue\n",
    "    except Exception:\n",
    "        continue\n",
    "    #handle page that does not actually exist\n",
    "    if len(page.read())==0:\n",
    "        print \"the above page does not exist\"\n",
    "        continue\n",
    "    else:\n",
    "        soup = BeautifulSoup(page)\n",
    "        #find all links from the current \"page\"\n",
    "        all_links= soup.find_all(\"a\")\n",
    "        #ensure all links are valid\n",
    "        link_inside= [link.get(\"href\") for link in all_links if link.get(\"href\") is not None]\n",
    "        link_inside= [link if link.startswith('http') or link.startswith('www') else s + link for link in link_inside]\n",
    "        #do not append links from starter links\n",
    "        link_inside = [link for link in link_inside if link not in site_web]\n",
    "        #do not append links from previous records\n",
    "        link_inside = [link for link in link_inside if link not in all_page]\n",
    "        #do not append links in the current records\n",
    "        link_inside = [link for link in link_inside if link not in all_page1]\n",
    "        for k in np.unique(link_inside):\n",
    "            print k, len(all_page1)\n",
    "            all_page1.append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep unique pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73016"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_page1=np.unique(all_page1)\n",
    "all_page1=list(all_page1)\n",
    "len(all_page1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter to remove social media links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72894"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=filter(lambda x: 'linkedin' in x or 'facebook' in x or 'twitter' in x or 'youtube' in x or 'http://www.meteo-senegal.net' in x, all_page1)\n",
    "for k in site_web:\n",
    "    f.append(k)\n",
    "all_page1= [x for x in all_page1 if x not in f]\n",
    "len(all_page1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and save a dataframe with \"all_page1\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://311.inburlingtonvt.us/index.php?title=9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://50.undp.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://50.undp.org/en/#future</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://50.undp.org/en/#timeline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://50.undp.org/en/africa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link\n",
       "0  http://311.inburlingtonvt.us/index.php?title=9...\n",
       "1                                 http://50.undp.org\n",
       "2                      http://50.undp.org/en/#future\n",
       "3                    http://50.undp.org/en/#timeline\n",
       "4                       http://50.undp.org/en/africa"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=all_page1, index= range(len(all_page1)), columns=['link'])\n",
    "df.to_csv(\"base_links.csv\",sep=\";\",encoding=\"utf-8\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for keywords related to flood events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_page1=pd.read_csv(\"base_links.csv\",sep=\";\")\n",
    "all_page1=list(all_page1.link)\n",
    "all_page=pd.read_csv(\"all_page.csv\",sep=\";\")\n",
    "all_page=list(all_page.link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flood_kwds='(meteo|evacués|evacuation|SOS|crise humanitaire|sinistres|précipitations|pluviométri|bidonville|'\n",
    "flood_kwds=flood_kwds+'montée des eaux|flood|zone risque|remblayage|effondrement|engorgement|débordé|digue'\n",
    "flood_kwds=flood_kwds+'|submergé|éclatement barrage|rupture barrage|glissement terrain|climatique|inondé|inondables|'\n",
    "flood_kwds= flood_kwds+'inondation|inondations|pluies|pluie|banlieu|bas-quartiers)'\n",
    "#2nd iter\n",
    "flood_page_from_all_page1= [x for x in all_page1 if re.search(flood_kwds,x) is not None]\n",
    "#1st iter\n",
    "flood_page_from_all_page= [x for x in all_page if re.search(flood_kwds,x) is not None]\n",
    "flood_page=flood_page_from_all_page + flood_page_from_all_page1\n",
    "flood_page=np.unique(flood_page)\n",
    "len(flood_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results ==>181 links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=flood_page, index= range(len(flood_page)), columns=['link'])\n",
    "df.to_csv(\"potential_flood_events.csv\",sep=\";\",encoding=\"utf-8\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
